---
title: "Machine Learning: Pumping Iron"
author: "Dan Simerlink"
date: "December 21, 2015"
output: html_document
---


Clear all variables from the work space
---------------------------------------

```{r Clear Variables}
rm(list=ls())
```

Load the relevant libraries
---------------------------
```{r Load Libraries, echo=TRUE, results="hide", message=FALSE}
library(caret)
library(data.table)
library(gbm)
library(ggplot2)
library(htmltools)
library(knitrBootstrap)
library(neuralnet)
library(parallel)
library(plyr)
library(randomForest)
library(rattle)
library(RColorBrewer)
library(Rcpp)
library(reshape2)
library(rmarkdown)
library(rpart)
library(rpart.plot)
library(splines)
library(survival)
library(xtable)
```

Document the environment
------------------------
```{r Document Environment}
sessionInfo()
```

Project Overview
================
Devices such as Jawbone Up, Nike FuelBand, and Fitbit make it possible to inexpensively collect a large amount of data about personal activity. These type of devices are used by a group of enthusiasts who regularly take measurements about themselves to improve their health or to find patterns in their behavior. The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbbell. "Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." 

In order to predict whether the exercise was done correctly or incorrectly, three machine learning algorithms were tested (decision tress, random forests, and generalized boosted regression). After comparing the accuracy, total out of sample error, and elapsed system time, it was concluded that the random forests machine learning method was the best choice for this data. Also after examining each of the variables, it was determiend that roll_belt, yaw_belt, and pitch_foream were the three most important variables. 

Sources: 
http://groupware.les.inf.puc-rio.br/har
https://class.coursera.org/predmachlearn-034/human_grading/view/courses/975204/assessments/4/submissions 
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3sQGwpUPF

Load, Clean, Prepare, and Examine the Data
===========================
The data is loaded into a data frame, and is then partitioned, checked for near zero variances, and then visually inspected.

Load the Data into a Data frame
------------------------------
After the data is loaded into a data frame, the integrity of the data is checked by comparing the column headings of the "trainingData" and the "submit20" data. (The "test" data set was renamed to "submit20" to differentiate it from the testing data) The only difference between the two data frames was found in column #160. Next the unnecessary columns and the columns containing NAs were deleted, which resulted in a reduction from 160 columns to 54 columns. 
```{r Load Data, echo=TRUE, results="hide"}
trainingData <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
submit20 <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
#Confirm that the column names of test_data match the column names of submit20
sameName <- colnames(trainingData) == colnames(submit20)
# display the names that don't match
colnames(trainingData)[sameName==FALSE]
colnames(submit20)[sameName==FALSE]

#Delete the unnecesary columns
trainingData2 <- (trainingData[ , c(2, 8:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)])
submit20_2 <-  (submit20[ , c(2, 8:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)])
```

Partition the data, and check for near zero variances
-----------------------------------------------------
The data frame trainingData is partitioned into 60% training data and 40% testing data. The data are also checked for near zero variances to ensure that it can be used as valid predictors. The results from the following code show that there were no near zero variances.

Source: Retrieved 11/14/2015 from http://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/ 
```{r Partition Data and NZV, echo=TRUE}
# Partition the trainingData into 60% training data and 40% testing data. 
set.seed(2345)
inTrain <- createDataPartition(y=trainingData2$classe, p=0.6, list=FALSE)
training <- trainingData2[inTrain, ]
testing <- trainingData2[-inTrain, ]

# Check for Nar Zero Variance in the data sets
nzvTraining <- nearZeroVar(training, saveMetrics = TRUE) 
#Remove the comment from the following line to print the nzv table
#knitr::kable(nzvTraining, digits = 2, caption = "Near Zero Variances. Training")
nzvTesting <- nearZeroVar(testing, saveMetrics = TRUE) 
#Remove the comment from the following line to print the nzv table
#knitr::kable(nzvTesting, digits = 2, caption = "Near Zero Variances. Testing")
```

Visually Examine the Data
=========================
Next, data was visually examined via histograms, and many of the histograms displayed a multi-modal distribution. This is to be expected given that the data includes the use of the correct method, and several uses of the incorrect method. 
```{r Histograms, fig.width = 7, fig.height = 11, warning = FALSE, error=FALSE, message=FALSE, results='asis'}
trainBox <- melt(training[,c(2:54)])
ggplot(trainBox,aes(x = value)) + 
    facet_wrap(~variable,nrow = 9, ncol = 6, scales = "free_x") + 
    geom_histogram() 
```

Train the Machine Learning Model
================================
The training data will be processed via the three machine learning models (decision tress, random forests, and generalized boosted regression). Each of these three machine learning models uses a different approach; short explanations of each approach are included at the beginning of the relevant section. 

Decision Trees
----------------
In this section, the decision trees are visually depicted, and then a confusion matrix is created to determine the accuracy of the machine learning model. "Decision trees classify instances by sorting them down the tree from the root to some leaf node. Each node in the tree specifies a test of some attribute of the instance and each branch descending from the node corresponds to one of the possible values for this attribute."

Source: Retrieved on 11/30/2015 from http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf 

```{r Train Decision Trees, warning=FALSE, error=FALSE, fig.width=7}
set.seed(2345)
decisionTreeTrain <- rpart(classe ~ ., data=training, method = "class")
fancyRpartPlot(decisionTreeTrain)

#Create Confusion Matrix
predictionsDecisionTreeTrain <- predict(decisionTreeTrain, training, type = "class")
decisionTreeConfusionMatrixTrain <- confusionMatrix(predictionsDecisionTreeTrain, training$classe)
decisionTreeConfusionMatrixTrain
```

Random Forest
----------------
The random forest model is visually depicted, and then a confusion matrix is created to determine the accuracy of this machine learning model. "The random forest starts with a standard machine learning technique called a "decision tree"...In a decision tree, an input is entered at the top and as it traverses down the tree the data gets bucketed into smaller and smaller sets...[One strength is that] random forest run times are quite fast, and they are able to deal with unbalanced and missing data. Random Forest weaknesses are that when used for regression they cannot predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy." Note: To avoid overfitting the data using random forests, see http://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting

Source: Retrieved 11/30/2015 from https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/

```{r Train Random Forests}
set.seed(2345)
randomForestTrain <- randomForest(classe ~ ., data=training)
predictions_ranfor_train <- predict(randomForestTrain, type = "class")
randomForestConfusionMatrixTrain <- confusionMatrix(predictions_ranfor_train, training$classe)

plot(randomForestTrain, main = "")
title(main="Random Forests") 

randomForestConfusionMatrixTrain
```

Generalized Boosted Regression
--------------------------------
The Generalized Boosted Regression (GBR) model is visually depicted, and then a confusion matrix is created to determine the accuracy of this machine learning model.
"[In] GBMs, the learning procedure consecutively fits new models to provide a more accurate estimate of the response variable...Includes regression methods for least squares, absolute loss, logistic, Poisson etc..."


Sources: 

Retrieved on 11/30/2015 from http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/

Retrieved on 11/30/2015 from https://cran.r-project.org/web/packages/gbm/gbm.pdf 

```{r Train GBR, message=FALSE}
set.seed(2345)
gbrControlTrain <- trainControl(method = "repeatedcv", number = 5,repeats = 1)
gbmFit1Train <- train(classe ~ ., data=training, method = "gbm", trControl = gbrControlTrain, verbose = FALSE)
gbmFinalModel1Train  <- gbmFit1Train$finalModel
gbmPredictionTrain <- predict(gbmFit1Train, newdata=training)
gbmConfusionMatrixTrain <- confusionMatrix(gbmPredictionTrain, training$classe)

par(mar = rep(2, 4))
plot(gbmFit1Train, ylim=c(0.75, 1))

gbmConfusionMatrixTrain
```


Test the Machine Learning Models
================================
Now that each of the three machine learning models have been executed with the training data, the exact same model will be used with the testing data. 

Decision Trees
----------------
```{r Test Decision Trees, warning=FALSE, error=FALSE, fig.width=7}
set.seed(2345)
decisionTreeTest <- rpart(classe ~ ., data=testing, method = "class")
#Create Confusion Matrix
predictionsDecisionTreeTest <- predict(decisionTreeTest, testing, type = "class")
decisionTreeConfusionMatrixTest <- confusionMatrix(predictionsDecisionTreeTest, testing$classe)
decisionTreeConfusionMatrixTest
```

Random Forests
----------------
```{r Test Random Forests}
set.seed(2345)
randomForestTest <- randomForest(classe ~ ., data=testing)
predictionsRandomForestTest <- predict(randomForestTest, type = "class")
randomForestConfusionMatrixTest <- confusionMatrix(predictionsRandomForestTest, testing$classe)
randomForestConfusionMatrixTest
```

Generalized Boosted Regression
--------------------------------
```{r Test GBR, message=FALSE}
set.seed(2345)
gbrControlTest <- trainControl(method = "repeatedcv", number = 5,repeats = 1)
gbmFit1Test <- train(classe ~ ., data=testing, method = "gbm", trControl = gbrControlTest, verbose = FALSE)
gbmFinalModel1Test <- gbmFit1Test$finalModel
gbmPredictionTest <- predict(gbmFit1Test, newdata=testing)
gbmConfusionMatrixTest <- confusionMatrix(gbmPredictionTest, testing$classe)
gbmConfusionMatrixTest
#gbmConfusionMatrixTest$overall
```

Compare the Machine Learning Algorithms
================================================================
The three machine learning models have now been trained and tested, a comparison is created using the accuracy, the total out-of-sample error, and the elapsed system time for the test. The random forest method had the greatest accuracy, the lowest out of sample error, and the lowest elapsed system time. Note: Out-of-sample Error can be derived as 1-FinalAccuracy. 

```{r Accuracy and Out of Sample Error, warning=FALSE}
row1 <- c("Training Accuracy", decisionTreeConfusionMatrixTrain$overall[1], randomForestConfusionMatrixTrain$overall[1], gbmConfusionMatrixTrain$overall[1])
row2 <- c("Testing Accuracy",decisionTreeConfusionMatrixTest$overall[1], randomForestConfusionMatrixTest$overall[1], gbmConfusionMatrixTest$overall[1])
row3 <- c("Total Out of Sample Error", (1-((decisionTreeConfusionMatrixTrain$overall[1]+decisionTreeConfusionMatrixTest$overall[1])/2)), (1-((randomForestConfusionMatrixTrain$overall[1]+randomForestConfusionMatrixTest$overall[1])/2)), (1-((gbmConfusionMatrixTrain$overall[1]+gbmConfusionMatrixTest$overall[1])/2)))
#Calculate Elapsed System Time for "Test" to Determine the Speed of Execution
systemTimeDecisionTreeConfusionMatrixTest <- system.time( replicate(100000, decisionTreeConfusionMatrixTest))
systemTimeRandomForestConfusionMatrixTest <- system.time( replicate(100000, randomForestConfusionMatrixTest))
systemTimeGbmConfusionMatrixTest <- system.time( replicate(100000, gbmConfusionMatrixTest))
row4 <- c("Elapsed System Time for Test", (systemTimeDecisionTreeConfusionMatrixTest[3]), (systemTimeRandomForestConfusionMatrixTest[3]), (systemTimeGbmConfusionMatrixTest[3]))

Accuracy <- as.data.frame(rbind(row1, row2, row3, row4))
names(Accuracy) <- c("", "Decision Trees", "Random Forests","Generalized Boosted Regression")
knitr::kable(Accuracy, row.names=FALSE, caption = "Accuracy")
```

Determine the Important Variables
=============================
The data used for this study contained many variables, and the following graph and table depict the most important ones. The importance was ranked using the "wrappers around the importance functions from the randomForest and party packages, respectively. Partial Least Squares (PLS): the variable importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the sums of squares across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the sums of squares. Recursive Partitioning: The reduction in the loss function (e.g. mean squared error) attributed to each variable at each split is tabulated and the sum is returned. Also, since there may be candidate variables that are important but are not used in a split, the top competing variables are also tabulated at each split."


Source: Retrieved 11/30/2015 from http://www.inside-r.org/packages/cran/caret/docs/varimp 

```{r Determine which Variables are Most Important, warning=FALSE}

varImpPlot(randomForestTest, sort=TRUE, n.var=min(10, nrow(randomForestTest$importance)),
           type=NULL, class=NULL, scale=TRUE, 
           main=deparse(substitute("Important Variables")))

#class(randomForestTest)
randomForestImportance <- varImp(randomForestTest, by.tree=FALSE, count=TRUE, sort.default(randomForestTest$Overall, decreasing = FALSE))
randomForestImportance$max <- apply(randomForestImportance, 1, max)
randomForestImportanceSorted <- randomForestImportance[order(-randomForestImportance$max),] 
knitr::kable(randomForestImportanceSorted, row.names=TRUE)
```


